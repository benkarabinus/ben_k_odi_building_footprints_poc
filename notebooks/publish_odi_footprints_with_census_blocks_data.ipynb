{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a53d6e-594c-4896-b97b-80982a32440d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Notebook Overview"
    }
   },
   "source": [
    "\n",
    "# Export Building Footprints to GeoParquet and Shapefile\n",
    "\n",
    "This notebook exports building footprints data from a Databricks silver table to:\n",
    "* **GeoParquet** format (for efficient geospatial data storage)\n",
    "* **Zipped Shapefile** format (for compatibility with GIS tools like Esri)\n",
    "\n",
    "Data is partitioned by `county_fips` and written to a Unity Catalog volume.\n",
    "\n",
    "## Parameters\n",
    "* **source_table**: Fully qualified table name (catalog.schema.table)\n",
    "* **volume_path**: Unity Catalog volume path (e.g., `/Volumes/catalog/schema/volume_name`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43c3f733-e8ad-4636-b106-dc025123605c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Required Packages"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Note specific package versions can be pinned for reporoducability\n",
    "# All packages already exist in the serverless runtime\n",
    "# Additional packages are installed here for illustration\n",
    "%pip install geopandas pyarrow shapely --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa103fa4-c047-426e-8bf8-de219ac9dc4d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Libraries"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from shapely import wkb\n",
    "import zipfile\n",
    "import tempfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e05b3de2-0f7e-4bcd-a4b6-689a50dd6685",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configure Paths"
    }
   },
   "outputs": [],
   "source": [
    "# Get parameters\n",
    "#dbutils.widgets.text(\"source_table\", \"odi_datalake.odi_silver.building_footprints_with_blocks_and_places_\")\n",
    "#dbutils.widgets.text(\"volume_path\", \"/Volumes/odi_datalake/odi_gold/global_ml_building_footprints\")\n",
    "year = dbutils.widgets.get(\"tiger_year\")\n",
    "source_table = dbutils.widgets.get(\"source_table\") + year\n",
    "volume_path = dbutils.widgets.get(\"volume_path\")\n",
    "\n",
    "\n",
    "# Create subdirectories for parquet and shapefile outputs\n",
    "parquet_path = f\"{volume_path}/parquet/{year}\"\n",
    "shp_path = f\"{volume_path}/shp/{year}\"\n",
    "\n",
    "print(f\"Source table: {source_table}\")\n",
    "print(f\"Output paths:\")\n",
    "print(f\"  - GeoParquet: {parquet_path}\")\n",
    "print(f\"  - Shapefiles: {shp_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "980a7bc6-ea1a-47d4-b336-213e81f4c03d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get Distinct Counties"
    }
   },
   "outputs": [],
   "source": [
    "# Read all data for the specified year (single query instead of 57+ queries)\n",
    "print(f\"Loading data for year {year}...\")\n",
    "all_data_df = spark.table(source_table)\n",
    "\n",
    "# Get distinct counties from the data\n",
    "counties_df = all_data_df \\\n",
    "    .select(\"county_fips\") \\\n",
    "    .distinct() \\\n",
    "    .filter(F.col(\"county_fips\").isNotNull()) \\\n",
    "    .orderBy(\"county_fips\")\n",
    "\n",
    "counties = [row.county_fips for row in counties_df.collect()]\n",
    "print(f\"Found {len(counties)} counties to process for year {year}\")\n",
    "print(f\"Counties: {counties[:10]}{'...' if len(counties) > 10 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f006a20-3aa6-4f2b-b592-a033c13951f9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clear Output Directories"
    }
   },
   "outputs": [],
   "source": [
    "# Clear existing files in output directories to avoid accumulation from previous runs\n",
    "print(f\"Cleaning output directories...\")\n",
    "\n",
    "try:\n",
    "    # Remove and recreate parquet directory\n",
    "    dbutils.fs.rm(parquet_path, recurse=True)\n",
    "    print(f\"  ✓ Cleared {parquet_path}\")\n",
    "except:\n",
    "    print(f\"  ℹ {parquet_path} doesn't exist yet\")\n",
    "\n",
    "try:\n",
    "    # Remove and recreate shapefile directory\n",
    "    dbutils.fs.rm(shp_path, recurse=True)\n",
    "    print(f\"  ✓ Cleared {shp_path}\")\n",
    "except:\n",
    "    print(f\"  ℹ {shp_path} doesn't exist yet\")\n",
    "\n",
    "print(\"Ready to process counties.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf658082-8997-4491-bd1b-97ae4f917adb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Process Each County"
    }
   },
   "outputs": [],
   "source": [
    "# Create output directories if they don't exist\n",
    "dbutils.fs.mkdirs(parquet_path)\n",
    "dbutils.fs.mkdirs(shp_path)\n",
    "\n",
    "# Process each county and write to GeoParquet and Shapefile\n",
    "for index, county in enumerate(counties):\n",
    "    print(f\"Processing county_fips {county} ({index + 1}/{len(counties)})...\")\n",
    "    \n",
    "    # Filter cached data for this county (no additional table query needed)\n",
    "    county_df = all_data_df.filter(F.col(\"county_fips\") == county)\n",
    "    \n",
    "    # Convert to Pandas DataFrame\n",
    "    pdf = county_df.toPandas()\n",
    "    \n",
    "    # Convert geometry column from WKB to shapely geometries using vectorized operation\n",
    "    # Extract WKB bytes from Databricks geometry type, then use shapely's from_wkb\n",
    "    pdf['geometry'] = gpd.GeoSeries.from_wkb(pdf['geometry'].apply(lambda x: x.wkb if x is not None else None))\n",
    "    \n",
    "    # Create GeoDataFrame with EPSG:4326 (WGS84) coordinate reference system\n",
    "    gdf = gpd.GeoDataFrame(pdf, geometry='geometry', crs='EPSG:4326')\n",
    "    \n",
    "    # Filter out GeometryCollection types (as in original code)\n",
    "    gdf = gdf[gdf.geometry.geom_type != 'GeometryCollection']\n",
    "    \n",
    "    # Define file prefix\n",
    "    file_prefix = f\"county_fips_{county}\"\n",
    "    \n",
    "    # Write to GeoParquet (overwrites by default)\n",
    "    parquet_file = f\"{parquet_path}/{file_prefix}.parquet\"\n",
    "    gdf.to_parquet(parquet_file)\n",
    "    print(f\"  ✓ Wrote {len(gdf)} records to {parquet_file}\")\n",
    "    \n",
    "    # Write to zipped shapefile\n",
    "    # .shz suffix triggers GDAL to write zipped shapefile automatically\n",
    "    temp_shz = f\"/tmp/{file_prefix}.shz\"\n",
    "    gdf.to_file(temp_shz)\n",
    "    \n",
    "    # Copy to volume with .zip extension (for Esri compatibility)\n",
    "    shp_file = f\"{shp_path}/{file_prefix}.zip\"\n",
    "    \n",
    "    # Use shutil.copy to copy the binary file directly\n",
    "    shutil.copy(temp_shz, shp_file)\n",
    "    \n",
    "    # Clean up temp file\n",
    "    os.remove(temp_shz)\n",
    "    print(f\"  ✓ Wrote shapefile to {shp_file}\")\n",
    "\n",
    "print(f\"\\n✅ Processing complete! Processed {len(counties)} counties for year {year}.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "publish_odi_footprints_with_census_blocks_data",
   "widgets": {
    "source_table": {
     "currentValue": "odi_datalake.odi_silver.building_footprints_with_blocks_and_places_",
     "nuid": "1359f058-4112-457c-83a9-ba683e341510",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "odi_datalake.odi_silver.building_footprints_with_blocks_and_places_",
      "label": null,
      "name": "source_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "odi_datalake.odi_silver.building_footprints_with_blocks_and_places_",
      "label": null,
      "name": "source_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "tiger_year": {
     "currentValue": "2020",
     "nuid": "c814470e-ac6f-41e8-8f26-beaccbad7b07",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2023",
      "label": "",
      "name": "tiger_year",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2023",
      "label": "",
      "name": "tiger_year",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "volume_path": {
     "currentValue": "/Volumes/odi_datalake/odi_gold/global_ml_building_footprints",
     "nuid": "4f1285d5-ecc9-4880-8af7-29807ebdfdc3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/odi_datalake/odi_gold/global_ml_building_footprints",
      "label": null,
      "name": "volume_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Volumes/odi_datalake/odi_gold/global_ml_building_footprints",
      "label": null,
      "name": "volume_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
